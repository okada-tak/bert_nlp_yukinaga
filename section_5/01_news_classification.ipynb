{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_news_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+jlwIO2oHuQ1CSlhpydtp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/bert_nlp/blob/main/section_5/01_news_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrgegdDZjf8E"
      },
      "source": [
        "# 日本語文章の分類\n",
        "\n",
        "日本語のデータセットでBERTのモデルをファインチューニングし、ニュースの分類を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6moZnLFkFwr"
      },
      "source": [
        "## ライブラリのインストール\n",
        "ライブラリTransformers、およびnlpをインストールします。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qg6t5nnBjqs"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install nlp\n",
        "!pip install datasets\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcHOX9LyZc2g"
      },
      "source": [
        "## Google ドライブとの連携  \n",
        "以下のコードを実行し、認証コードを使用してGoogle ドライブをマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h7BA67Ed5wT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zliYGLC5g0h2"
      },
      "source": [
        "## データセットの読み込み\n",
        "Googleドライブに保存されている、ニュースのデータセットを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPV3qCYs9STS"
      },
      "source": [
        "import glob  # ファイルの取得に使用\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/My Drive/bert_nlp/section_5/text/\"  # フォルダの場所を指定\n",
        "\n",
        "dir_files = os.listdir(path=path)\n",
        "dirs = [f for f in dir_files if os.path.isdir(os.path.join(path, f))]  # ディレクトリ一覧\n",
        "\n",
        "text_label_data = []  # 文章とラベルのセット\n",
        "dir_count = 0  # ディレクトリ数のカウント\n",
        "file_count= 0  # ファイル数のカウント\n",
        "\n",
        "for i in range(len(dirs)):\n",
        "    dir = dirs[i]\n",
        "    files = glob.glob(path + dir + \"/*.txt\")  # ファイルの一覧\n",
        "    dir_count += 1\n",
        "\n",
        "    for file in files:\n",
        "        if os.path.basename(file) == \"LICENSE.txt\":\n",
        "            continue\n",
        "\n",
        "        with open(file, \"r\") as f:\n",
        "            text = f.readlines()[3:]\n",
        "            text = \"\".join(text)\n",
        "            text = text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"})) \n",
        "            text_label_data.append([text, i])\n",
        "\n",
        "        file_count += 1\n",
        "        print(\"\\rfiles: \" + str(file_count) + \"dirs: \" + str(dir_count), end=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LADy70wOgyXg"
      },
      "source": [
        "## データの保存\n",
        "データを訓練データとテストデータに分割し、csvファイルとしてGoogle Driveに保存します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIyvN2MT4Unl"
      },
      "source": [
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "news_train, news_test =  train_test_split(text_label_data, shuffle=True)  # 訓練用とテスト用に分割\n",
        "news_path = \"/content/drive/My Drive/bert_nlp/section_5/\"\n",
        "\n",
        "with open(news_path+\"news_train.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(news_train)\n",
        "\n",
        "with open(news_path+\"news_test.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(news_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsgQNMJxpBnW"
      },
      "source": [
        "## モデルとTokenizerの読み込み\n",
        "日本語の事前学習済みモデルと、これと紐づいたTokenizerを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R0HK29fHrf3"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
        "\n",
        "sc_model = BertForSequenceClassification.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", num_labels=9)\n",
        "sc_model.cuda()\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWCmm2TjqToE"
      },
      "source": [
        "## データセットの読み込み\n",
        "保存されたニュースのデータを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfEnNpv9HuXI"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n",
        "    \n",
        "news_path = \"/content/drive/My Drive/bert_nlp/section_5/\"\n",
        "\n",
        "train_data = load_dataset(\"csv\", data_files=news_path+\"news_train.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n",
        "train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))\n",
        "train_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "\n",
        "test_data = load_dataset(\"csv\", data_files=news_path+\"news_test.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n",
        "test_data = test_data.map(tokenize, batched=True, batch_size=len(test_data))\n",
        "test_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y6Fcqmy2rG2"
      },
      "source": [
        "## 評価用の関数\n",
        "`sklearn.metrics`を使用し、モデルを評価するための関数を定義します。  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plAZjdkG0FdV"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(result):\n",
        "    labels = result.label_ids\n",
        "    preds = result.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjLqAVy7z0T3"
      },
      "source": [
        "## Trainerの設定\n",
        "Trainerクラス、およびTrainingArgumentsクラスを使用して、訓練を行うTrainerの設定を行います。 \n",
        "https://huggingface.co/transformers/main_classes/trainer.html   \n",
        "https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhaexaAOI3kV"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./results\",\n",
        "    num_train_epochs = 2,\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size = 32,\n",
        "    warmup_steps = 500,  # 学習係数が0からこのステップ数で上昇\n",
        "    weight_decay = 0.01,  # 重みの減衰率\n",
        "    # evaluate_during_training = True,  # ここの記述はバージョンによっては必要ありません\n",
        "    logging_dir = \"./logs\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = sc_model,\n",
        "    args = training_args,\n",
        "    compute_metrics = compute_metrics,\n",
        "    train_dataset = train_data,\n",
        "    eval_dataset = test_data,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0F5nXKpSCnS"
      },
      "source": [
        "## モデルの訓練\n",
        "設定に基づきファインチューニングを行います。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29fkN4UcI4jm"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c76zhkQVS2xZ"
      },
      "source": [
        "## モデルの評価\n",
        "Trainerの`evaluate()`メソッドによりモデルを評価します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIgke21zI6l_"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EFwqzLRUhaB"
      },
      "source": [
        "## TensorBoardによる結果の表示\n",
        "TensorBoardを使って、logsフォルダに格納された学習過程を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vv39tuDJq5n"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-BscHjHxs0H"
      },
      "source": [
        "## モデルの保存\n",
        "訓練済みのモデルを保存します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvwVcXuIyH7V"
      },
      "source": [
        "news_path = \"/content/drive/My Drive/bert_nlp/section_5/\"\n",
        "\n",
        "sc_model.save_pretrained(news_path)\n",
        "tokenizer.save_pretrained(news_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZuJCZBK0RJx"
      },
      "source": [
        "## モデルの読み込み\n",
        "保存済みのモデルを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWtcQRuP0X45"
      },
      "source": [
        "loaded_model = BertForSequenceClassification.from_pretrained(news_path) \n",
        "loaded_model.cuda()\n",
        "loaded_tokenizer = BertJapaneseTokenizer.from_pretrained(news_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq2zZ99R3Hs7"
      },
      "source": [
        "## 日本語ニュースの分類\n",
        "読み込んだモデルを使ってニュースを分類します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFOIjY511WVK"
      },
      "source": [
        "import glob  # ファイルの取得に使用\n",
        "import os\n",
        "import torch\n",
        "\n",
        "category = \"movie-enter\"\n",
        "sample_path = \"/content/drive/My Drive/bert_nlp/section_5/text/\"  # フォルダの場所を指定\n",
        "files = glob.glob(sample_path + category + \"/*.txt\")  # ファイルの一覧\n",
        "file = files[12]  # 適当なニュース\n",
        "\n",
        "dir_files = os.listdir(path=sample_path)\n",
        "dirs = [f for f in dir_files if os.path.isdir(os.path.join(sample_path, f))]  # ディレクトリ一覧\n",
        "\n",
        "with open(file, \"r\") as f:\n",
        "    sample_text = f.readlines()[3:]\n",
        "    sample_text = \"\".join(sample_text)\n",
        "    sample_text = sample_text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"})) \n",
        "\n",
        "print(sample_text)\n",
        "\n",
        "max_length = 512\n",
        "words = loaded_tokenizer.tokenize(sample_text)\n",
        "word_ids = loaded_tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "word_tensor = torch.tensor([word_ids[:max_length]])  # テンソルに変換\n",
        "\n",
        "x = word_tensor.cuda()  # GPU対応\n",
        "y = loaded_model(x)  # 予測\n",
        "pred = y[0].argmax(-1)  # 最大値のインデックス\n",
        "print(\"result:\", dirs[pred])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}