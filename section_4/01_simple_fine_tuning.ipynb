{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_simple_fine_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4F3CFb1+lvySnXQV8THMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/bert_nlp/blob/main/section_4/01_simple_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv454lBK1YCc"
      },
      "source": [
        "# シンプルなファインチューニング\n",
        "最小限のコードでファインチューニングを実装します。  \n",
        "事前学習済みのモデルに、何回か追加で訓練を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Ozfz3NhltP"
      },
      "source": [
        "## ライブラリのインストール\n",
        "ライブラリTransformersをインストールします。  \n",
        "https://huggingface.co/transformers/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mDYVlb-sqi"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nP8RaW-SLD-"
      },
      "source": [
        "## モデルの読み込み\n",
        "`BertForSequenceClassification`により、事前学習済みのモデルを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36bQQblhwpqT"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "sc_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", return_dict=True)\n",
        "print(sc_model.state_dict().keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQV44nAdV82l"
      },
      "source": [
        "## 最適化アルゴリズム\n",
        "今回は、最適化アルゴリズムに`AdamW`を採用します。  \n",
        "`AdamW`は、オリジナルのAdamの重みの減衰に関する式を変更したものです。  \n",
        "https://arxiv.org/abs/1711.05101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVoPZhUPw4o7"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(sc_model.parameters(), lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2KV0V8ajTz"
      },
      "source": [
        "## Tokenizerの設定\n",
        "`BertTokenizer`により文章を単語に分割し、idに変換します。  \n",
        "`BertForSequenceClassification`のモデルの訓練時には入力の他にAttention maskを渡す必要があるのですが、`BertTokenizer`によりこちらも得ることができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bSSCHNExo-I"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "sentences = [\"I love baseball.\", \"I hate baseball.\"]\n",
        "tokenized = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "print(tokenized)\n",
        "\n",
        "x = tokenized[\"input_ids\"]\n",
        "attention_mask = tokenized[\"attention_mask\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMhbOWRXXT0n"
      },
      "source": [
        "## ファインチューニング\n",
        "事前に学習済みのモデルに対して、追加で訓練を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1MqceDP1IeR"
      },
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sc_model.train()\n",
        "t = torch.tensor([1,0])  # 文章の分類\n",
        "\n",
        "weight_record = []  # 重みを記録\n",
        "\n",
        "for i in range(100):\n",
        "    y = sc_model(x, attention_mask=attention_mask)\n",
        "    loss = F.cross_entropy(y.logits, t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    weight = sc_model.state_dict()[\"bert.encoder.layer.11.output.dense.weight\"][0][0].item()\n",
        "    weight_record.append(weight)\n",
        "\n",
        "plt.plot(range(len(weight_record)), weight_record)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWcBnNXOYy32"
      },
      "source": [
        "追加の訓練により、重みが調整されていく様子が確認できます。"
      ]
    }
  ]
}